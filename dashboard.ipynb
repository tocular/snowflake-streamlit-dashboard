{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard Data Preparation\n",
    "\n",
    "This notebook pulls data from Snowflake to be fed into a Streamlit dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tochi/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from snowflake_engine import SnowflakeEngine # custom SQLAlchemy Engine Wrapper\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected as: OCTOPYTH0N\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Snowflake connection using custom engine\n",
    "sf = SnowflakeEngine()\n",
    "sf.test_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available tables (8):\n",
      "- CUSTOMER\n",
      "- LINEITEM\n",
      "- NATION\n",
      "- ORDERS\n",
      "- PART\n",
      "- PARTSUPP\n",
      "- REGION\n",
      "- SUPPLIER\n"
     ]
    }
   ],
   "source": [
    "# implementation with SnowflakeEngine\n",
    "tables_df = sf.query(\"\"\"SHOW TABLES IN SCHEMA SNOWFLAKE_SAMPLE_DATA.TPCH_SF1\"\"\")\n",
    "\n",
    "print(f\"\\nAvailable tables ({len(tables_df)}):\")\n",
    "for table in tables_df['name'].values:\n",
    "    print(f\"- {table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Revenue Trend with Moving Average\n",
    "Monthly revenue data with 3-month moving average for trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 43 months of revenue data\n",
      "Date range: 1995-01-01 00:00:00 to 1998-07-01 00:00:00\n",
      "\n",
      "First 3 rows:\n",
      "       month       revenue  order_count  unique_customers   revenue_ma3  \\\n",
      "0 1995-01-01  2.946587e+09        19472             17525  2.946587e+09   \n",
      "1 1995-02-01  2.689142e+09        17721             16116  2.817865e+09   \n",
      "2 1995-03-01  2.910048e+09        19313             17333  2.848593e+09   \n",
      "\n",
      "   prev_month_revenue  mom_growth_pct  \n",
      "0                 NaN             NaN  \n",
      "1        2.946587e+09       -8.737066  \n",
      "2        2.689142e+09        8.214749  \n"
     ]
    }
   ],
   "source": [
    "# Query 1: Revenue Trend with Moving Average\n",
    "# Aggregating by month to reduce data volume\n",
    "revenue_trend_query = \"\"\"\n",
    "WITH monthly_revenue AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', O_ORDERDATE) as month,\n",
    "        SUM(O_TOTALPRICE) as revenue,\n",
    "        COUNT(*) as order_count,\n",
    "        COUNT(DISTINCT O_CUSTKEY) as unique_customers\n",
    "    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\n",
    "    WHERE O_ORDERDATE >= '1995-01-01'  -- Limiting to last 3-4 years to reduce data\n",
    "        AND O_ORDERDATE < '1998-08-01'  -- Exclude incomplete August 1998 data\n",
    "    GROUP BY DATE_TRUNC('month', O_ORDERDATE)\n",
    ")\n",
    "SELECT \n",
    "    month,\n",
    "    revenue,\n",
    "    order_count,\n",
    "    unique_customers,\n",
    "    -- 3-month moving average\n",
    "    AVG(revenue) OVER (\n",
    "        ORDER BY month \n",
    "        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "    ) as revenue_ma3,\n",
    "    -- Month-over-month growth\n",
    "    LAG(revenue, 1) OVER (ORDER BY month) as prev_month_revenue,\n",
    "    ((revenue - LAG(revenue, 1) OVER (ORDER BY month)) / \n",
    "     NULLIF(LAG(revenue, 1) OVER (ORDER BY month), 0) * 100) as mom_growth_pct\n",
    "FROM monthly_revenue\n",
    "ORDER BY month\n",
    "\"\"\"\n",
    "\n",
    "df_revenue_trend = sf.query(revenue_trend_query)\n",
    "# convert date to datetime object\n",
    "df_revenue_trend['month'] = pd.to_datetime(df_revenue_trend['month']) \n",
    "\n",
    "print(f\"Retrieved {len(df_revenue_trend)} months of revenue data\")\n",
    "print(f\"Date range: {df_revenue_trend['month'].min()} to {df_revenue_trend['month'].max()}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df_revenue_trend.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Product Revenue by Country\n",
    "Monthly revenue contribution by product (part) for each country and aggregated for the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved 5,965,025 rows of product revenue data\n",
      "Countries: 26 (including World)\n",
      "Unique product types: 5\n",
      "Date range: 1995-01-01 00:00:00 to 1998-07-01 00:00:00\n",
      "\n",
      "Sample data:\n",
      "       month  country product_type  product_id   revenue\n",
      "0 1995-01-01  ALGERIA        BRASS      167817  37696.20\n",
      "1 1995-01-01  ALGERIA        BRASS      107282  39967.68\n",
      "2 1995-01-01  ALGERIA        BRASS      180482  34374.56\n",
      "3 1995-01-01  ALGERIA        BRASS      165483  44905.92\n",
      "4 1995-01-01  ALGERIA        BRASS      181585  69996.36\n",
      "5 1995-01-01  ALGERIA        BRASS      187896   5951.67\n",
      "6 1995-01-01  ALGERIA        BRASS      197529  63434.28\n",
      "7 1995-01-01  ALGERIA        BRASS      111767  53362.80\n",
      "8 1995-01-01  ALGERIA        BRASS       73130  46331.46\n",
      "9 1995-01-01  ALGERIA        BRASS      127509  41485.50\n",
      "\n",
      "Unique product types: ['BRASS', 'COPPER', 'NICKEL', 'STEEL', 'TIN']\n"
     ]
    }
   ],
   "source": [
    "# Query 6: Product Revenue by Country - Time Series\n",
    "# Getting monthly revenue contribution by part for each country + World aggregate\n",
    "product_revenue_query = \"\"\"\n",
    "-- Country-level product revenue\n",
    "WITH country_product_revenue AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', o.O_ORDERDATE) as month,\n",
    "        n.N_NAME as country,\n",
    "        p.P_TYPE as product_type,\n",
    "        p.P_PARTKEY as product_id,\n",
    "        SUM(l.L_EXTENDEDPRICE) as revenue\n",
    "    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.LINEITEM l\n",
    "    JOIN SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS o \n",
    "        ON l.L_ORDERKEY = o.O_ORDERKEY\n",
    "    JOIN SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.PART p \n",
    "        ON l.L_PARTKEY = p.P_PARTKEY\n",
    "    JOIN SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER c \n",
    "        ON o.O_CUSTKEY = c.C_CUSTKEY\n",
    "    JOIN SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION n \n",
    "        ON c.C_NATIONKEY = n.N_NATIONKEY\n",
    "    WHERE o.O_ORDERDATE >= '1995-01-01'\n",
    "        AND o.O_ORDERDATE < '1998-08-01'\n",
    "    GROUP BY DATE_TRUNC('month', o.O_ORDERDATE), n.N_NAME, p.P_TYPE, p.P_PARTKEY\n",
    "),\n",
    "-- World aggregate product revenue\n",
    "world_product_revenue AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', o.O_ORDERDATE) as month,\n",
    "        'World' as country,\n",
    "        p.P_TYPE as product_type,\n",
    "        p.P_PARTKEY as product_id,\n",
    "        SUM(l.L_EXTENDEDPRICE) as revenue\n",
    "    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.LINEITEM l\n",
    "    JOIN SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS o \n",
    "        ON l.L_ORDERKEY = o.O_ORDERKEY\n",
    "    JOIN SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.PART p \n",
    "        ON l.L_PARTKEY = p.P_PARTKEY\n",
    "    WHERE o.O_ORDERDATE >= '1995-01-01'\n",
    "        AND o.O_ORDERDATE < '1998-08-01'\n",
    "    GROUP BY DATE_TRUNC('month', o.O_ORDERDATE), p.P_TYPE, p.P_PARTKEY\n",
    ")\n",
    "-- Combine country and world data\n",
    "SELECT * FROM country_product_revenue\n",
    "UNION ALL\n",
    "SELECT * FROM world_product_revenue\n",
    "ORDER BY month, country, product_type\n",
    "\"\"\"\n",
    "\n",
    "df_product_revenue = sf.query(product_revenue_query)\n",
    "df_product_revenue['month'] = pd.to_datetime(df_product_revenue['month'])\n",
    "\n",
    "# Extract only the last word from product_type (e.g., \"STANDARD POLISHED COPPER\" -> \"COPPER\")\n",
    "df_product_revenue['product_type'] = df_product_revenue['product_type'].str.split().str[-1]\n",
    "\n",
    "print(f\"\\nRetrieved {len(df_product_revenue):,} rows of product revenue data\")\n",
    "print(f\"Countries: {df_product_revenue['country'].nunique()} (including World)\")\n",
    "print(f\"Unique product types: {df_product_revenue['product_type'].nunique()}\")\n",
    "print(f\"Date range: {df_product_revenue['month'].min()} to {df_product_revenue['month'].max()}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(df_product_revenue.head(10))\n",
    "print(f\"\\nUnique product types: {sorted(df_product_revenue['product_type'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geographic Anomaly Detection\n",
    "Monthly metrics by country with statistical baselines for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query 7: Geographic Anomaly Detection\n",
    "# Pull monthly metrics by country with statistical baselines\n",
    "import numpy as np\n",
    "\n",
    "geographic_anomaly_query = \"\"\"\n",
    "WITH monthly_country_metrics AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', o.O_ORDERDATE) as month,\n",
    "        n.N_NAME as country,\n",
    "        n.N_NATIONKEY as country_key,\n",
    "        r.R_NAME as region,\n",
    "        \n",
    "        -- Core metrics\n",
    "        COUNT(DISTINCT o.O_ORDERKEY) as order_count,\n",
    "        COUNT(DISTINCT c.C_CUSTKEY) as unique_customers,\n",
    "        SUM(o.O_TOTALPRICE) as total_revenue,\n",
    "        AVG(o.O_TOTALPRICE) as avg_order_value,\n",
    "        \n",
    "        -- Operational metrics\n",
    "        SUM(CASE WHEN o.O_ORDERSTATUS = 'F' THEN 1 ELSE 0 END) as fulfilled_orders,\n",
    "        SUM(CASE WHEN o.O_ORDERSTATUS = 'O' THEN 1 ELSE 0 END) as open_orders,\n",
    "        \n",
    "        -- Calculate orders per customer\n",
    "        COUNT(DISTINCT o.O_ORDERKEY) / NULLIF(COUNT(DISTINCT c.C_CUSTKEY), 0) as orders_per_customer\n",
    "        \n",
    "    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS o\n",
    "    JOIN SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER c \n",
    "        ON o.O_CUSTKEY = c.C_CUSTKEY\n",
    "    JOIN SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION n \n",
    "        ON c.C_NATIONKEY = n.N_NATIONKEY\n",
    "    JOIN SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.REGION r \n",
    "        ON n.N_REGIONKEY = r.R_REGIONKEY\n",
    "    WHERE o.O_ORDERDATE >= '1995-01-01'\n",
    "        AND o.O_ORDERDATE < '1998-08-01'\n",
    "    GROUP BY \n",
    "        DATE_TRUNC('month', o.O_ORDERDATE),\n",
    "        n.N_NAME,\n",
    "        n.N_NATIONKEY,\n",
    "        r.R_NAME\n",
    "),\n",
    "\n",
    "-- Calculate historical statistics for each country (for anomaly detection)\n",
    "country_baselines AS (\n",
    "    SELECT \n",
    "        country,\n",
    "        country_key,\n",
    "        region,\n",
    "        \n",
    "        -- Revenue statistics\n",
    "        AVG(total_revenue) as avg_revenue,\n",
    "        STDDEV(total_revenue) as stddev_revenue,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY total_revenue) as q1_revenue,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY total_revenue) as q3_revenue,\n",
    "        \n",
    "        -- Order count statistics\n",
    "        AVG(order_count) as avg_orders,\n",
    "        STDDEV(order_count) as stddev_orders,\n",
    "        \n",
    "        -- Customer statistics\n",
    "        AVG(unique_customers) as avg_customers,\n",
    "        STDDEV(unique_customers) as stddev_customers,\n",
    "        \n",
    "        -- Average order value statistics\n",
    "        AVG(avg_order_value) as avg_aov,\n",
    "        STDDEV(avg_order_value) as stddev_aov,\n",
    "        \n",
    "        -- Fulfillment rate statistics\n",
    "        AVG(fulfilled_orders * 100.0 / NULLIF(order_count, 0)) as avg_fulfillment_rate,\n",
    "        STDDEV(fulfilled_orders * 100.0 / NULLIF(order_count, 0)) as stddev_fulfillment_rate\n",
    "        \n",
    "    FROM monthly_country_metrics\n",
    "    GROUP BY country, country_key, region\n",
    ")\n",
    "\n",
    "-- Join current metrics with baselines\n",
    "SELECT \n",
    "    m.month,\n",
    "    m.country,\n",
    "    m.country_key,\n",
    "    m.region,\n",
    "    m.order_count,\n",
    "    m.unique_customers,\n",
    "    m.total_revenue,\n",
    "    m.avg_order_value,\n",
    "    m.fulfilled_orders,\n",
    "    m.open_orders,\n",
    "    m.orders_per_customer,\n",
    "    \n",
    "    -- Baseline statistics\n",
    "    b.avg_revenue,\n",
    "    b.stddev_revenue,\n",
    "    b.q1_revenue,\n",
    "    b.q3_revenue,\n",
    "    b.avg_orders,\n",
    "    b.stddev_orders,\n",
    "    b.avg_customers,\n",
    "    b.stddev_customers,\n",
    "    b.avg_aov,\n",
    "    b.stddev_aov,\n",
    "    b.avg_fulfillment_rate,\n",
    "    b.stddev_fulfillment_rate,\n",
    "    \n",
    "    -- Calculate Z-scores for anomaly detection\n",
    "    CASE \n",
    "        WHEN b.stddev_revenue > 0 \n",
    "        THEN (m.total_revenue - b.avg_revenue) / b.stddev_revenue\n",
    "        ELSE 0\n",
    "    END as revenue_zscore,\n",
    "    \n",
    "    CASE \n",
    "        WHEN b.stddev_orders > 0 \n",
    "        THEN (m.order_count - b.avg_orders) / b.stddev_orders\n",
    "        ELSE 0\n",
    "    END as orders_zscore,\n",
    "    \n",
    "    CASE \n",
    "        WHEN b.stddev_customers > 0 \n",
    "        THEN (m.unique_customers - b.avg_customers) / b.stddev_customers\n",
    "        ELSE 0\n",
    "    END as customers_zscore,\n",
    "    \n",
    "    CASE \n",
    "        WHEN b.stddev_aov > 0 \n",
    "        THEN (m.avg_order_value - b.avg_aov) / b.stddev_aov\n",
    "        ELSE 0\n",
    "    END as aov_zscore,\n",
    "    \n",
    "    -- IQR-based anomaly detection for revenue\n",
    "    b.q3_revenue + 1.5 * (b.q3_revenue - b.q1_revenue) as revenue_upper_fence,\n",
    "    b.q1_revenue - 1.5 * (b.q3_revenue - b.q1_revenue) as revenue_lower_fence,\n",
    "    \n",
    "    -- Flag if revenue is outside IQR fences\n",
    "    CASE \n",
    "        WHEN m.total_revenue > (b.q3_revenue + 1.5 * (b.q3_revenue - b.q1_revenue))\n",
    "            OR m.total_revenue < (b.q1_revenue - 1.5 * (b.q3_revenue - b.q1_revenue))\n",
    "        THEN 1\n",
    "        ELSE 0\n",
    "    END as revenue_iqr_outlier\n",
    "\n",
    "FROM monthly_country_metrics m\n",
    "JOIN country_baselines b ON m.country = b.country\n",
    "ORDER BY m.month DESC, m.country\n",
    "\"\"\"\n",
    "\n",
    "# Execute query\n",
    "df_geographic_anomalies = sf.query(geographic_anomaly_query)\n",
    "df_geographic_anomalies['month'] = pd.to_datetime(df_geographic_anomalies['month'])\n",
    "\n",
    "# Sort by country and month for MoM calculations\n",
    "df_geographic_anomalies = df_geographic_anomalies.sort_values(['country', 'month'])\n",
    "\n",
    "# 1. Month-over-month change detection\n",
    "df_geographic_anomalies['revenue_mom_change'] = df_geographic_anomalies.groupby('country')['total_revenue'].pct_change() * 100\n",
    "df_geographic_anomalies['orders_mom_change'] = df_geographic_anomalies.groupby('country')['order_count'].pct_change() * 100\n",
    "\n",
    "# 2. Flag sudden changes (>30% swing)\n",
    "df_geographic_anomalies['revenue_spike'] = df_geographic_anomalies['revenue_mom_change'].abs() > 30\n",
    "df_geographic_anomalies['orders_spike'] = df_geographic_anomalies['orders_mom_change'].abs() > 30\n",
    "\n",
    "# 3. Correlation breakdown detection\n",
    "# Expected revenue based on order count and avg order value\n",
    "df_geographic_anomalies['expected_revenue'] = df_geographic_anomalies['order_count'] * df_geographic_anomalies['avg_aov']\n",
    "df_geographic_anomalies['revenue_deviation_pct'] = (\n",
    "    (df_geographic_anomalies['total_revenue'] - df_geographic_anomalies['expected_revenue']) / \n",
    "    df_geographic_anomalies['expected_revenue'] * 100\n",
    ")\n",
    "df_geographic_anomalies['correlation_breakdown'] = df_geographic_anomalies['revenue_deviation_pct'].abs() > 20\n",
    "\n",
    "# 4. Fulfillment rate anomalies\n",
    "df_geographic_anomalies['fulfillment_rate'] = (\n",
    "    df_geographic_anomalies['fulfilled_orders'] / df_geographic_anomalies['order_count'] * 100\n",
    ")\n",
    "df_geographic_anomalies['fulfillment_anomaly'] = (\n",
    "    (df_geographic_anomalies['fulfillment_rate'] < \n",
    "     df_geographic_anomalies['avg_fulfillment_rate'] - 2 * df_geographic_anomalies['stddev_fulfillment_rate']) |\n",
    "    (df_geographic_anomalies['fulfillment_rate'] > \n",
    "     df_geographic_anomalies['avg_fulfillment_rate'] + 2 * df_geographic_anomalies['stddev_fulfillment_rate'])\n",
    ")\n",
    "\n",
    "# 5. Z-score based anomalies (threshold = 2.0)\n",
    "df_geographic_anomalies['revenue_anomaly'] = df_geographic_anomalies['revenue_zscore'].abs() > 2.0\n",
    "df_geographic_anomalies['orders_anomaly'] = df_geographic_anomalies['orders_zscore'].abs() > 2.0\n",
    "df_geographic_anomalies['customers_anomaly'] = df_geographic_anomalies['customers_zscore'].abs() > 2.0\n",
    "df_geographic_anomalies['aov_anomaly'] = df_geographic_anomalies['aov_zscore'].abs() > 2.0\n",
    "\n",
    "# 6. Calculate composite anomaly score (0-100)\n",
    "# Weight different anomaly types\n",
    "weights = {\n",
    "    'revenue_zscore': 0.30,\n",
    "    'orders_zscore': 0.20,\n",
    "    'customers_zscore': 0.15,\n",
    "    'aov_zscore': 0.10,\n",
    "    'revenue_mom_change': 0.15,\n",
    "    'revenue_deviation_pct': 0.10\n",
    "}\n",
    "\n",
    "# Normalize each component to 0-100 scale\n",
    "df_geographic_anomalies['revenue_component'] = np.clip(df_geographic_anomalies['revenue_zscore'].abs() * 20, 0, 100) * weights['revenue_zscore']\n",
    "df_geographic_anomalies['orders_component'] = np.clip(df_geographic_anomalies['orders_zscore'].abs() * 20, 0, 100) * weights['orders_zscore']\n",
    "df_geographic_anomalies['customers_component'] = np.clip(df_geographic_anomalies['customers_zscore'].abs() * 20, 0, 100) * weights['customers_zscore']\n",
    "df_geographic_anomalies['aov_component'] = np.clip(df_geographic_anomalies['aov_zscore'].abs() * 20, 0, 100) * weights['aov_zscore']\n",
    "df_geographic_anomalies['mom_component'] = np.clip(df_geographic_anomalies['revenue_mom_change'].abs() / 3, 0, 100) * weights['revenue_mom_change']\n",
    "df_geographic_anomalies['deviation_component'] = np.clip(df_geographic_anomalies['revenue_deviation_pct'].abs() / 2, 0, 100) * weights['revenue_deviation_pct']\n",
    "\n",
    "# Sum weighted components\n",
    "df_geographic_anomalies['anomaly_score'] = (\n",
    "    df_geographic_anomalies['revenue_component'] +\n",
    "    df_geographic_anomalies['orders_component'] +\n",
    "    df_geographic_anomalies['customers_component'] +\n",
    "    df_geographic_anomalies['aov_component'] +\n",
    "    df_geographic_anomalies['mom_component'] +\n",
    "    df_geographic_anomalies['deviation_component']\n",
    ")\n",
    "\n",
    "# 7. Categorize severity - QUARTILE-BASED RANGES\n",
    "df_geographic_anomalies['anomaly_severity'] = pd.cut(\n",
    "    df_geographic_anomalies['anomaly_score'],\n",
    "    bins=[-np.inf, 25, 50, 75, np.inf],\n",
    "    labels=['Normal', 'Minor', 'Moderate', 'Severe']\n",
    ")\n",
    "\n",
    "# 8. List specific anomaly types detected\n",
    "def get_anomaly_types(row):\n",
    "    anomalies = []\n",
    "    if row['revenue_anomaly']:\n",
    "        direction = 'spike' if row['revenue_zscore'] > 0 else 'drop'\n",
    "        anomalies.append(f\"Revenue {direction} (Z={row['revenue_zscore']:.1f})\")\n",
    "    if row['orders_anomaly']:\n",
    "        direction = 'spike' if row['orders_zscore'] > 0 else 'drop'\n",
    "        anomalies.append(f\"Order volume {direction} (Z={row['orders_zscore']:.1f})\")\n",
    "    if row['customers_anomaly']:\n",
    "        direction = 'increase' if row['customers_zscore'] > 0 else 'decrease'\n",
    "        anomalies.append(f\"Customer count {direction}\")\n",
    "    if row['aov_anomaly']:\n",
    "        anomalies.append(\"AOV shift\")\n",
    "    if row['revenue_spike'] and not pd.isna(row['revenue_mom_change']):\n",
    "        anomalies.append(f\"Revenue MoM: {row['revenue_mom_change']:.1f}%\")\n",
    "    if row['correlation_breakdown']:\n",
    "        anomalies.append(\"Rev-orders correlation breakdown\")\n",
    "    if row['fulfillment_anomaly']:\n",
    "        anomalies.append(\"Fulfillment rate anomaly\")\n",
    "    \n",
    "    return ', '.join(anomalies) if anomalies else 'None'\n",
    "\n",
    "df_geographic_anomalies['anomaly_types'] = df_geographic_anomalies.apply(get_anomaly_types, axis=1)\n",
    "\n",
    "# Country name to ISO-3 code mapping\n",
    "COUNTRY_CODE_MAP = {\n",
    "    'ALGERIA': 'DZA',\n",
    "    'ARGENTINA': 'ARG',\n",
    "    'BRAZIL': 'BRA',\n",
    "    'CANADA': 'CAN',\n",
    "    'EGYPT': 'EGY',\n",
    "    'ETHIOPIA': 'ETH',\n",
    "    'FRANCE': 'FRA',\n",
    "    'GERMANY': 'DEU',\n",
    "    'INDIA': 'IND',\n",
    "    'INDONESIA': 'IDN',\n",
    "    'IRAN': 'IRN',\n",
    "    'IRAQ': 'IRQ',\n",
    "    'JAPAN': 'JPN',\n",
    "    'JORDAN': 'JOR',\n",
    "    'KENYA': 'KEN',\n",
    "    'MOROCCO': 'MAR',\n",
    "    'MOZAMBIQUE': 'MOZ',\n",
    "    'PERU': 'PER',\n",
    "    'CHINA': 'CHN',\n",
    "    'ROMANIA': 'ROU',\n",
    "    'SAUDI ARABIA': 'SAU',\n",
    "    'VIETNAM': 'VNM',\n",
    "    'RUSSIA': 'RUS',\n",
    "    'UNITED KINGDOM': 'GBR',\n",
    "    'UNITED STATES': 'USA'\n",
    "}\n",
    "\n",
    "# Add ISO-3 country codes for mapping\n",
    "df_geographic_anomalies['country_code'] = df_geographic_anomalies['country'].map(COUNTRY_CODE_MAP)\n",
    "\n",
    "# Check for missing country codes\n",
    "missing = df_geographic_anomalies[df_geographic_anomalies['country_code'].isna()]['country'].unique()\n",
    "if len(missing) > 0:\n",
    "    print(f\"\\nWarning: Missing country codes for: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### How the Anomaly Score is Calculated\n",
    "\n",
    "The **Anomaly Score** (0-100) is a composite metric that identifies unusual patterns in country-level performance. It combines multiple detection methods with different weights:\n",
    "\n",
    "#### Score Calculation:\n",
    "\n",
    "Each component is normalized to a 0-100 scale and multiplied by its weight:\n",
    "- **Revenue z-score** : 30% weight\n",
    "- **Orders z-score** : 20% weight\n",
    "- **Customer count z-score** : 15% weight\n",
    "- **Average Order Value (AOV) z-score** : 10% weight\n",
    "- **MoM Revenue Change** : 15% weight\n",
    "- **Difference between actual and expected revenue based on order count and average AOV** : 10% weight\n",
    "\n",
    "**Final Score** = Sum of all weighted components (0-100)\n",
    "\n",
    "#### Severity Categories:\n",
    "- **Normal** (0-25)\n",
    "- **Minor** (25-50)\n",
    "- **Moderate** (50-75)\n",
    "- **Severe** (75-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data to CSV Files\n",
    "Save all datasets to the tables folder for use in Streamlit dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create tables directory if it doesn't exist\n",
    "#os.makedirs('tables', exist_ok=True)\n",
    "\n",
    "# Export all dataframes to CSV\n",
    "# 1. Revenue Trend\n",
    "df_revenue_trend.to_csv('tables/revenue_trend.csv', index=False)\n",
    "\n",
    "# 2. Product revenue by country\n",
    "df_product_revenue.to_csv('tables/product_revenue_by_country.csv', index=False)\n",
    "\n",
    "# 3. Geographic Anomalies\n",
    "df_geographic_anomalies.to_csv('tables/geographic_anomalies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed\n"
     ]
    }
   ],
   "source": [
    "# Close the Snowflake connection\n",
    "sf.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
